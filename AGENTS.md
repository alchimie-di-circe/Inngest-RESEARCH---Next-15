# AGENTS.md - Testing & Development Orchestration

> **Status**: Master Agent Guidelines for Research & Publishing Suite  
> **Last Updated**: January 16, 2026  
> **For**: Claude Code, Droid, Kilocode, Gemini CLI (all AI agent orchestrators)  
> **Context**: Used with TestSprite MCP + Wallaby MCP + Dagger container-use MCP

---

## ğŸ¯ CORE PRINCIPLE

**Never run tests locally on the Mac Air. Always use sandboxes.**

```
User's Mac (8GB RAM, 250GB HD)     â† PROTECTED
    â†“
Development & coding (normal)      â† Full IDE experience
    â†“
Test execution (ALWAYS delegated)  â† TestSprite MCP, Wallaby MCP, CI/CD
```

---

## ğŸš« LOCAL MACHINE RESTRICTIONS (Cloud-First Development)

**This project is FULL CLOUD. The Mac Air is for code editing only.**

### Development Environment: GitHub Codespace

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  GitHub Codespace = Your Cloud Dev Machine                  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  âœ… npm install          â†’ Runs in cloud, not on Mac       â”‚
â”‚  âœ… npm run dev          â†’ localhost:3000 forwarded        â”‚
â”‚  âœ… npm run build        â†’ Build in cloud                  â”‚
â”‚  âœ… npx prisma migrate   â†’ DB operations in cloud          â”‚
â”‚  âœ… Full testing suite   â†’ Jest, Playwright, etc.          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**How to start:**

1. GitHub repo â†’ Code â†’ Codespaces â†’ "New codespace"
2. Wait for devcontainer setup (~2 min first time)
3. In Codespace terminal: `npm install && npm run dev`
4. Click forwarded port â†’ Opens localhost:3000 in browser

### Local Mac Air: Code Editing Only

```bash
# âœ… ALLOWED on Mac Air (lightweight, no install)
git status / diff / commit / push   # Git operations
npx prettier --write <files>        # Format (uses npx cache)
npx tsc --noEmit                    # Type check only
code . / cursor .                   # Open editor

# âŒ NEVER on Mac Air (heavy operations)
npm install       # â†’ Use Codespace instead
npm run build     # â†’ Use Codespace instead
npm run dev       # â†’ Use Codespace instead
docker compose up # â†’ Use Codespace instead
npx prisma migrate deploy  # â†’ Use CI/CD or Codespace
```

### Agent Decision: "Should I run this locally?"

```
User/Task asks: "Run npm install", "Build the project", or "Start dev server"
    â†“
Agent Response:
    "This project uses cloud-first development. I won't run npm install/build
     on your Mac Air.

     To test the app:
     1. Open GitHub Codespace: gh codespace create -r <repo>
     2. In Codespace terminal: npm install && npm run dev
     3. Click the forwarded port link to open in browser

     Need me to help with something else while you set up Codespace?"
```

### Recommended Workflow: Task â†’ Branch â†’ PR â†’ Review

```yaml
Task assigned: 1. Create branch locally       â†’ git checkout -b feat/task-xyz
  2. Write code on Mac Air       â†’ Edit files in VSCode/Cursor
  3. Format & type-check         â†’ npx prettier --write . && npx tsc --noEmit
  4. Commit & push               â†’ git commit -m "..." && git push

Testing (Pre-PR): 5. Open GitHub Codespace       â†’ Test app locally in cloud
  6. npm install && npm run dev  â†’ Verify feature works
  7. Run integration tests       â†’ Use TestSprite MCP in Codespace

PR & Review: 8. Create PR                   â†’ gh pr create
  9. Reviewer opens Codespace    â†’ Tests in their cloud env
  10. Review â†’ Approve
  11. Merge                      â†’ Squash & merge to main
```

### Future: Vercel Preview (Optional Enhancement)

When ready (after stable main branch), add Vercel for automatic PR previews:

```
User pushes branch
    â†“
Vercel auto-builds & deploys preview
    â†“
PR comment: "Preview: https://inngest-research-feat-xyz.vercel.app"
    â†“
Reviewers click link instead of opening Codespace
```

**Setup when needed:**

- `vercel link` in Codespace
- Configure env vars in Vercel Dashboard
- All subsequent pushes auto-deploy

---

## ğŸ“¦ Git Sparse-Checkout Configuration

To avoid downloading heavy directories (like `node_modules/`) while keeping all project files (.claude, .taskmaster, .factory, etc.):

```bash
# Setup sparse-checkout (one-time, during initial clone)
git sparse-checkout init --cone
git sparse-checkout set --no-cone '*' '!node_modules'

# Verify configuration
git sparse-checkout list
# Should show:
#   *
#   !node_modules
```

**Why sparse-checkout?**

- **Saves disk space**: Excludes `node_modules/` (884MB+) from local checkout
- **Keeps all configs**: ALL project files visible (.claude, .taskmaster, .factory, .gemini, .codex, docs, etc.)
- **Git still tracks everything**: Just doesn't materialize node_modules locally (it's in .gitignore anyway)
- **Cloud-first aligned**: You install packages in Codespace, not locally

**What gets checked out:**

```
âœ… Visible (checkout):
   .claude, .codex, .factory, .gemini, .taskmaster
   src, prisma, public, scripts, docs, e2e, tests
   .devcontainer, .github, .env.example, package.json, etc.

âŒ NOT visible (excluded):
   node_modules/ (won't be downloaded)
```

---

## ğŸ” AGENT DECISION TREE

When Droid/Claude Code/Kilocode receives a development request:

```
Request: "Implement feature X with tests"
    â†“
Agent (You reading this):
    1. Check: Is this a unit test edit/write?
       â†’ YES: Use Wallaby MCP (local IDE integration or cloud)
       â†’ NO: Continue

    2. Check: Is this an integration/E2E test?
       â†’ YES: Use TestSprite MCP sandbox
       â†’ NO: Continue

    3. Check: Is this a feature implementation?
       â†’ YES: Code in local IDE (Codespaces or devcontainer)
       â†’ Then: Trigger TestSprite MCP for integration tests
       â†’ Then: CI/CD runs full suite

    4. Default: Never `npm test` locally. Always delegate.
```

---

## ğŸ§ª TEST RUNNER MATRIX

| Test Type       | Runner         | Where              | Config File                   | When       | Command              |
| --------------- | -------------- | ------------------ | ----------------------------- | ---------- | -------------------- |
| **Unit**        | Wallaby (MCP)  | Local IDE or Cloud | `.mcp/wallaby-config.json`    | On save    | `wallaby watch`      |
| **Unit** (CI)   | Jest           | GitHub Actions     | `jest.config.js`              | On push    | `npm test:ci`        |
| **Integration** | TestSprite MCP | Sandbox (headless) | `.mcp/testsprite-config.json` | Pre-commit | `mcp run testsprite` |
| **E2E**         | TestSprite MCP | Sandbox + Docker   | `.mcp/testsprite-config.json` | Pre-commit | `mcp run testsprite` |
| **Performance** | GitHub Actions | Server             | `.github/workflows/perf.yml`  | On push    | Auto                 |
| **Build**       | GitHub Actions | Server             | `.github/workflows/build.yml` | On push    | Auto                 |

---

## ğŸ“– DETAILED AGENT WORKFLOWS

### Scenario 1: Agent Implements a New Feature (Common)

**Request to Agent**:

```
"Implement deep-research-agent.ts with proper error handling and tests"
```

**Agent Execution Plan**:

```yaml
Step 1: Code Implementation (Local IDE)
  â””â”€ Create: src/inngest/deep-research-agent.ts
  â””â”€ Add types: src/types/research.ts (if needed)
  â””â”€ Code follows AgentKit Pattern 4 (multi-steps tools)
  â””â”€ Status: âœ… Typed, compilable

Step 2: Unit Test Writing (Wallaby MCP Coordination)
  â””â”€ Create: src/inngest/deep-research-agent.test.ts
  â””â”€ Write: Test search orchestration, parallel execution, error handling
  â””â”€ Pattern: Mocks for search_web tool + Neon DB
  â””â”€ Status: Tests FAIL (red) - as expected in TDD
  â””â”€ Wallaby MCP runs in IDE (watch mode)
  â””â”€ Agent watches Wallaby output in IDE console

Step 3: Implementation Loop
  â””â”€ Agent implements until unit tests PASS (green)
  â””â”€ No `npm test` locally - Wallaby handles it
  â””â”€ Status: Unit tests âœ… PASS

Step 4: Integration Test Creation (TestSprite MCP)
  â””â”€ Create: tests/integration/deep-research.test.ts
  â””â”€ Write: Real Neon DB, real Inngest functions
  â””â”€ Pattern: Setup test DB, run function, verify schema
  â””â”€ **Execution**: Delegate to TestSprite MCP
  â””â”€ Command: "mcp invoke testsprite --test tests/integration/deep-research.test.ts"
  â””â”€ TestSprite runs in sandbox â† ZERO local overhead
  â””â”€ Agent monitors: TestSprite output in logs/testsprite.log
  â””â”€ Status: Integration tests âœ… PASS

Step 5: Pre-Commit Checklist
  â””â”€ âœ… Prettier formatting applied (npx prettier --write <files>)
  â””â”€ âœ… Unit tests passing (Wallaby)
  â””â”€ âœ… Integration tests passing (TestSprite)
  â””â”€ âœ… TypeScript compilation succeeds
  â””â”€ âœ… No console errors
  â””â”€ âœ… Coverage meets thresholds (80% lines)

Step 6: Commit & Push
  â””â”€ git add src/inngest/deep-research-agent.ts tests/
  â””â”€ git commit -m "feat(Phase 1): Implement deep research agent with multi-step orchestration"
  â””â”€ git push
  â””â”€ Status: âœ… Ready for CI/CD

Step 7: CI/CD Verification (GitHub Actions - Automatic)
  â””â”€ GitHub automatically runs:
     â”œâ”€ npm run test:ci (all unit tests via Jest)
     â”œâ”€ npm run test:integration (all integration tests)
     â”œâ”€ npm run lint (ESLint + TypeScript check)
     â”œâ”€ npm run build (Next.js build)
  â””â”€ Status: âœ… All checks pass â†’ PR mergeable
```

**Key Points**:

- Agent NEVER runs `npm test` locally
- Wallaby MCP provides feedback in IDE (fast, watch mode)
- TestSprite MCP sandbox isolated from Mac Air (network request)
- CI/CD is final verification (parallel, server-side)

---

### Scenario 2: Agent Debugs a Failing Test

**Request to Agent**:

```
"Integration test for context-research-agent is failing. Debug and fix."
```

**Agent Execution Plan**:

````yaml
Step 1: Analyze Test Failure
  â””â”€ Look at CI output: github.com/repo/actions/run/XXX
  â””â”€ See error: "Cannot find brand_config table"
  â””â”€ Root cause: Migration not run in test setup

Step 2: Inspect Test Setup (Code)
  â””â”€ File: tests/integration/context-research.test.ts
  â””â”€ Look: setupFilesAfterEnv in jest.config.js
  â””â”€ Issue: DB migrations need to run BEFORE tests

Step 3: Fix: Update Test Setup
  â””â”€ Edit: tests/setup/integration.ts
  â””â”€ Add: Run migrations before tests
  â””â”€ Pattern:
     ```typescript
     beforeAll(async () => {
       await runMigrations(); // â† Add this
       await db.$connect();
     });
     ```

Step 4: Verify Fix (TestSprite MCP)
  â””â”€ Don't run locally - delegate:
  â””â”€ "mcp invoke testsprite --test tests/integration/context-research.test.ts"
  â””â”€ TestSprite runs in sandbox with fresh DB
  â””â”€ Output: Test PASSES âœ…
  â””â”€ Agent sees result in testsprite.log

Step 5: Commit Fix
  â””â”€ git add tests/setup/integration.ts
  â””â”€ git commit -m "fix(tests): Add DB migration setup for integration tests"
  â””â”€ git push
  â””â”€ CI/CD re-runs automatically
````

**Key Pattern**:

- Agent identifies issue from CI logs (not local reproduction)
- Agent fixes in IDE (edits test/setup files)
- Agent uses TestSprite MCP to verify (not local npm test)
- Agent commits + pushes (CI/CD final check)

---

### Scenario 3: Agent MCP server implementation (in Codebase)

**Request to Agent**:

```
"i want to add Canva MCP server to one of our Agentkit agents to let him create design using Canva MCP server tools. After implementing test some real possible user requests to the agent to check if it works without problems with our API. "
```

**Agent Execution Plan**:

````yaml
Step 1: Implement MCP Integration (Code)
  â””â”€ File: src/inngest/canva-agent.ts
  â””â”€ Pattern: AgentKit Pattern 6 (MCP integration)
  â””â”€ Code calls: MCP Canva server via agentkit.tools.canva_create()

Step 2: Unit Test (Wallaby MCP)
  â””â”€ File: src/inngest/canva-agent.test.ts
  â””â”€ Mock: MCP Canva responses
  â””â”€ Pattern:
     ```typescript
     jest.mock('@anthropic-sdk/tools', () => ({
       canva_create: jest.fn().mockResolvedValue({
         designUrl: 'https://canva.com/...'
       })
     }));
     ```
  â””â”€ Status: Tests PASS (mocked)

Step 3: Container Sandbox Test (Dagger MCP)
  â””â”€ For REAL MCP interaction, need container environment
  â””â”€ File: tests/e2e/canva-integration.test.ts
  â””â”€ Delegate to Dagger container-use:
  â””â”€ "mcp invoke container-use --test tests/e2e/canva-integration.test.ts"
  â””â”€ Container has MCP tools available
  â””â”€ Real Canva API calls (test account)
  â””â”€ Status: Design created âœ…

Step 4: Verify in Integration Test (TestSprite MCP)
  â””â”€ File: tests/integration/canva-agent.test.ts
  â””â”€ Test: Agent receives Canva API response, saves to DB
  â””â”€ Delegate to TestSprite MCP sandbox
  â””â”€ Status: Integration works âœ…

Step 5: Commit
  â””â”€ git add src/inngest/canva-agent.ts tests/
  â””â”€ git commit -m "feat(Phase 3): Implement Canva design agent with MCP integration"
````

**Key Tool Usage**:

- **Wallaby MCP**: Fast unit test feedback (mocks)
- **Dagger MCP**: Container sandboxes for real tool interaction
- **TestSprite MCP**: Integration tests (DB + API mocks)
- **CI/CD**: Final production verification

---

## ğŸ› ï¸ MCP SERVER SETUP

### 1. TestSprite MCP Configuration

**File: `.mcp/testsprite-config.json`**

```json
{
  "name": "testsprite",
  "type": "mcp-server",
  "command": "mcp-testsprite-server",
  "args": ["--token", "${TESTSPRITE_API_KEY}"],
  "env": {
    "TESTSPRITE_API_KEY": "ts_prod_xxxxx",
    "DATABASE_URL": "postgresql://test:test@localhost:5432/research_suite_test",
    "NODE_ENV": "test"
  },
  "capabilities": ["test_execution", "sandbox_management", "parallel_runs"],
  "options": {
    "timeout": 60000,
    "parallel_workers": 4,
    "cleanup": "always"
  }
}
```

**Available Commands**:

```bash
# Run specific test file
mcp invoke testsprite --test tests/integration/deep-research.test.ts

# Run all integration tests
mcp invoke testsprite --test tests/integration/**/*.test.ts

# Run with coverage
mcp invoke testsprite --test tests/e2e/*.test.ts --coverage

# Run with debugging enabled
mcp invoke testsprite --test tests/e2e/*.test.ts --debug --verbose

# Get test report
mcp invoke testsprite --report json --output testsprite-report.json
```

### 2. Wallaby MCP Configuration

**File: `.mcp/wallaby-config.json`**

```json
{
  "name": "wallaby",
  "type": "mcp-server",
  "command": "wallaby-server",
  "args": ["--port", "5555"],
  "env": {
    "NODE_ENV": "test",
    "WALLABY_PORT": "5555"
  },
  "capabilities": ["file_watcher", "test_execution", "coverage_reporting"],
  "options": {
    "watch_mode": true,
    "auto_run": true,
    "coverage_overlay": true
  }
}
```

**IDE Integration** (VSCode):

```json
// .vscode/settings.json
{
  "wallaby.testFramework": "jest",
  "wallaby.autoRun": "on-demand",
  "wallaby.showCoverageOnLoad": true
}
```

**Available Commands**:

```bash
# Watch unit tests (IDE integrated)
wallaby watch

# Watch specific tests only
wallaby watch --testNamePattern="context-research"

# Single run with coverage
wallaby run --coverage

# Debug a test
wallaby debug --test deep-research-agent.test.ts
```

### 3. Dagger Container-Use MCP Configuration

**File: `.mcp/container-use-config.json`**

```json
{
  "name": "container-use",
  "type": "mcp-server",
  "command": "container-use",
  "args": ["--engine", "docker"],
  "capabilities": ["container_execution", "mcp_tool_isolation", "docker_socket"],
  "options": {
    "image": "node:20-alpine",
    "mounts": {
      "/workspace": "${PWD}",
      "/mcp": "/mcp"
    },
    "network": "host"
  }
}
```

**Available Commands**:

```bash
# Run test in container with MCP tools available
mcp invoke container-use --test tests/e2e/canva-integration.test.ts

# Run full workflow in container
mcp invoke container-use --test tests/e2e/full-publish-flow.test.ts

# Interactive debugging
mcp invoke container-use --interactive --shell /bin/sh
```

---

## ğŸ“‹ TESTING WORKFLOW BY PHASE

### Phase 1: Deep Research (Agent Development)

**Workflow**:

```bash
# Development (you in IDE)
1. Agent writes: src/inngest/deep-research-agent.ts
2. Agent writes: src/inngest/deep-research-agent.test.ts (failing tests first - TDD)
3. Wallaby MCP watches + shows failures in IDE margin
4. Agent fixes implementation until Wallaby shows âœ… PASS

# Pre-commit (agent delegates)
5. Agent: "mcp invoke testsprite --test tests/integration/deep-research.test.ts"
6. TestSprite runs in sandbox â†’ agent waits for result
7. TestSprite: âœ… PASS or âŒ FAIL
   â†’ If PASS: Continue to step 8
   â†’ If FAIL: Agent edits test/code, re-invokes TestSprite

# Commit
8. Agent: git add/commit/push
9. CI/CD (automatic) runs full suite
10. GitHub Actions: âœ… All checks pass â†’ PR mergeable
```

**Agent Commands Cheat Sheet**:

```bash
# Development (watch unit tests)
wallaby watch

# Pre-commit (test integration)
mcp invoke testsprite --test tests/integration/deep-research.test.ts

# Full suite (CI equivalent locally)
npm run test:ci

# If tests fail, debug
mcp invoke testsprite --test tests/integration/deep-research.test.ts --debug --verbose
```

### Phase 2: Context Research (Multi-Agent Network)

**Special Pattern**: Multi-agent routing requires integration testing

```bash
# Agent writes agents first
1. Agent writes: src/inngest/context-research-agent.ts (multi-agent network)
2. Agent writes unit tests (mocks all sub-agents)
3. Wallaby MCP watches â†’ âœ… PASS for mocks

# Integration: Real multi-agent interaction
4. Agent: "mcp invoke testsprite --test tests/integration/context-research-network.test.ts"
5. TestSprite spins up real Inngest functions, real routing
6. TestSprite: âœ… Agents route correctly, all communicate

# Approval workflow (human-in-the-loop)
7. Agent tests approval gate separately (AgentKit Pattern 3)
8. Pattern: Pause Inngest â†’ show UI â†’ resume
9. File: tests/integration/approval-workflow.test.ts
10. TestSprite: âœ… Pause/resume works
```

### Phase 3: Content Generation (MCP Tools)

**Special Pattern**: MCP Canva requires container sandbox

```bash
# Unit tests (mocks)
1. Agent writes: src/inngest/canva-agent.test.ts (mock MCP)
2. Wallaby MCP: âœ… PASS (mocked responses)

# Container sandbox (real MCP)
3. Agent writes: tests/e2e/canva-integration.test.ts
4. Agent: "mcp invoke container-use --test tests/e2e/canva-integration.test.ts"
5. Container has MCP tools + Canva API access
6. Container: âœ… Real design created

# Integration (TestSprite)
7. Agent: "mcp invoke testsprite --test tests/integration/content-writer.test.ts"
8. TestSprite: âœ… Content + designs coordinated
```

### Phase 4: Publishing (Queue + Retries)

**Special Pattern**: Exponential backoff requires timeout testing

```bash
# Unit tests (queue logic)
1. Agent writes: src/inngest/queue-manager.test.ts
2. Test patterns:
   - Job enqueued â†’ pending
   - Job succeeds â†’ completed
   - Job fails â†’ retries with exponential backoff
3. Wallaby MCP: âœ… PASS (mocked external APIs)

# Integration (real queue)
4. Agent writes: tests/integration/queue-manager.test.ts
5. Pattern: Enqueue job, mock API failures, verify retry logic
6. Agent: "mcp invoke testsprite --test tests/integration/queue-manager.test.ts"
7. TestSprite: âœ… Retries work, exponential backoff verified

# E2E (full publish flow)
8. Agent writes: tests/e2e/full-publish-flow.test.ts
9. Pattern: Research â†’ Context â†’ Content â†’ Queue â†’ Publish
10. Agent: "mcp invoke testsprite --test tests/e2e/full-publish-flow.test.ts"
11. TestSprite: âœ… Full workflow succeeds
```

---

## ğŸš¨ ERROR HANDLING & ESCALATION

### Common Test Failures & Fixes

| Failure                 | Cause                          | Fix                                | Who                |
| ----------------------- | ------------------------------ | ---------------------------------- | ------------------ |
| `Cannot find DB table`  | Migration not run              | Add migration to test setup        | Agent              |
| `Timeout: 5000ms`       | Test too slow                  | Increase timeout or optimize       | Agent + TestSprite |
| `Mock not working`      | Mock declared after import     | Move mock before import            | Agent              |
| `Port 3000 in use`      | Dev server still running       | Kill process or use different port | Agent              |
| `MCP connection failed` | TestSprite/Wallaby not started | Check `.mcp/` config + API key     | Agent              |

### Escalation to Human (Rare)

**When to escalate**:

- ğŸ†˜ MCP server infrastructure issue (can't connect)
- ğŸ†˜ CI/CD system failure (GitHub Actions broken)
- ğŸ†˜ Database corruption (Neon issues)
- ğŸ†˜ Architectural design flaw (discovered during testing)

**How to escalate**:

```bash
# Save detailed logs
mcp invoke testsprite --test tests/integration/failing.test.ts --debug --verbose > testsprite-debug.log

# Report
echo "TestSprite MCP failed. See testsprite-debug.log for details."
echo "Escalating to human team."
```

---

## ğŸ“Š METRICS & MONITORING

### Metrics Tracked (Automatic)

| Metric                   | Tool                  | Target        | Action                        |
| ------------------------ | --------------------- | ------------- | ----------------------------- |
| Test Coverage            | Jest coverage reports | 80%+ lines    | PR blocked if < 80%           |
| Test Speed (unit)        | Wallaby MCP           | < 500ms       | Wallaby highlights slow tests |
| Test Speed (integration) | TestSprite MCP        | < 10s each    | TestSprite reports if > 10s   |
| Flaky Tests              | CI/CD retries         | 0 flaky tests | Disable & investigate         |
| Build Time               | GitHub Actions        | < 5 min       | Alert if > 5 min              |

### Dashboard (Monitoring)

```bash
# View test metrics
mcp invoke testsprite --report json --output metrics.json
cat metrics.json | grep -E "coverage|duration"

# CI/CD status
gh workflow view test.yml
gh run list --workflow test.yml --limit 10
```

---

## âœ… AGENT CHECKLIST (Before Committing)

```yaml
Before pushing to main:

â–¡ Code Implementation
  â–¡ Feature implemented in src/
  â–¡ Types updated (src/types/)
  â–¡ Imports use absolute paths (@/)
  â–¡ No console.log() or commented code
  â–¡ TypeScript: npx tsc --noEmit (no errors)

â–¡ Unit Tests (Wallaby MCP)
  â–¡ Tests written (follow existing patterns)
  â–¡ All unit tests PASS âœ… (Wallaby shows green)
  â–¡ Coverage >= 80% (Wallaby reports)
  â–¡ No skipped tests (unless documented)

â–¡ Integration Tests (TestSprite MCP)
  â–¡ Integration tests written (tests/integration/)
  â–¡ All integration tests PASS âœ…
  â–¡ "mcp invoke testsprite --test tests/integration/**/*.test.ts"
  â–¡ No flaky tests (retried, still pass)

â–¡ Code Quality
  â–¡ Prettier format applied: npx prettier --write <modified-files>
  â–¡ npm run lint (no errors)
  â–¡ npm run build (next.js builds successfully)
  â–¡ No security warnings (npm audit)

â–¡ Git
  â–¡ Commit message descriptive + follows convention
  â–¡ Related issue linked (if applicable)
  â–¡ No unrelated changes in commit

â–¡ Readiness
  â–¡ Reviewed code changes (readable, maintainable)
  â–¡ Thought through edge cases
  â–¡ Ready for CI/CD (GitHub Actions will verify)
```

---

## ğŸ“ REFERENCE GUIDE

### Command Quick Reference

```bash
# IDE Development (Local)
wallaby watch                # Watch unit tests (Wallaby MCP in IDE)
npm run dev                  # Start Next.js + Inngest
npm run inngest:dev          # Start Inngest CLI in separate terminal

# Testing (Delegated)
mcp invoke testsprite --test tests/integration/FILE.test.ts   # Integration test
mcp invoke testsprite --test tests/e2e/FILE.test.ts          # E2E test
mcp invoke container-use --test tests/e2e/FILE.test.ts       # With MCP tools

# Pre-Commit Verification (MANDATORY ORDER)
npx prettier --write <modified-files>  # Format code FIRST
npm run lint                 # ESLint
npx tsc --noEmit           # TypeScript check
npm run test:ci            # Jest (all tests)

# Debugging
mcp invoke testsprite --test FILE.test.ts --debug --verbose  # Detailed logs
wallaby debug --test FILE.test.ts                             # Debug in IDE
```

### File Locations

```
Testing Config:
  .mcp/testsprite-config.json      â† TestSprite MCP settings
  .mcp/wallaby-config.json         â† Wallaby MCP settings
  jest.config.js                   â† Jest configuration

Test Files:
  src/**/*.test.ts                 â† Unit tests (with code)
  tests/integration/               â† Integration tests
  tests/e2e/                       â† End-to-end tests
  tests/fixtures/                  â† Test data
  tests/setup/                     â† Setup files (DB, mocks)
```

### Key Patterns

**Mocking (Unit Tests)**:

```typescript
jest.mock('@lib/db');
const mockDb = db as jest.MockedObject<typeof db>;
```

**Integration Tests (TestSprite)**:

```typescript
describe('Feature', () => {
  beforeAll(async () => {
    await runMigrations(); // Real DB
    await db.$connect();
  });

  it('should work', async () => {
    // Real test against real DB
  });
});
```

**E2E Tests (TestSprite + Container)**:

```typescript
describe('Full Workflow', () => {
  it('should complete Phase 1 â†’ Phase 4', async () => {
    // Real agents, real APIs, real workflow
  });
}, 60000); // 60s timeout
```

---

## ğŸ”— RELATED DOCUMENTATION

- **PRD**: See `prd.txt` (Requirements & Architecture)
- **Architecture**: See `.claude/knowledge/project-architecture.md`
- **Inngest Patterns**: See `.claude/knowledge/inngest-patterns.md`
- **AgentKit Patterns**: See `.claude/knowledge/agentkit-advanced-patterns.md`
- **Factory.ai Token Efficiency**: https://docs.factory.ai/guides/power-user/token-efficiency

---

## ğŸ“ CHANGELOG

| Date       | Change                                    | Impact                                      |
| ---------- | ----------------------------------------- | ------------------------------------------- |
| 2026-01-25 | Add LOCAL MACHINE RESTRICTIONS section    | Cloud-first dev rules + Codespace workflow  |
| 2026-01-16 | Created AGENTS.md                         | Agent testing coordination established      |
| TBD        | Add Wallaby cloud integration             | Remove local Wallaby if cloud available     |
| TBD        | Add TestSprite failure analysis           | Auto-suggest fixes for common test failures |
| TBD        | Add Vercel Preview deployment integration | Auto-deploy PR previews (post-MVP)          |

---

**This file is the source of truth for AI agent testing workflows.**  
**Last Updated**: January 25, 2026  
**Status**: Production-Ready for Implementation
